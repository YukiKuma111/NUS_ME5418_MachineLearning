{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Vectorized Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from parallelEnv import parallelEnv\n",
    "from model_ref import Policy\n",
    "from ppo_ref import ppo_agent\n",
    "from storage import RolloutStorage\n",
    "from gym.vector import SyncVectorEnv\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from envs import make_vec_envs\n",
    "from utils import get_render_func\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "print('gym version: ', gym.__version__)\n",
    "print('torch version: ', torch.__version__)\n",
    "print('cuda version: ', torch.version.cuda)\n",
    "print('cuda available: ', torch.cuda.is_available())\n",
    "print(\"CUDA device count: \", torch.cuda.device_count())\n",
    "print(\"Current CUDA device: \", torch.cuda.current_device())\n",
    "\n",
    "seed = 0 \n",
    "gamma=0.99\n",
    "num_processes=16 \n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "print('device: ', device)\n",
    "\n",
    "envs = parallelEnv('Group24M4-v0', n=num_processes, seed=seed)\n",
    "max_steps = envs.max_steps\n",
    "\n",
    "# 实例化模型\n",
    "policy = Policy(envs.observation_space.shape, envs.action_space,\\\n",
    "        base_kwargs={'recurrent': True})\n",
    "# 加载预训练模型的参数\n",
    "actor_path = \"dir_save/20241031_105022_slope_good/we0_actor_final.pth\"\n",
    "critic_path = \"dir_save/20241031_105022_slope_good/we0_critic_final.pth\"\n",
    "critic_linear_path = \"dir_save/20241031_105022_slope_good/we0_critic_linear_final.pth\"\n",
    "policy.base.actor.load_state_dict(torch.load(actor_path))\n",
    "policy.base.critic.load_state_dict(torch.load(critic_path))\n",
    "policy.base.critic_linear.load_state_dict(torch.load(critic_linear_path))\n",
    "print(\"Successfully loaded pretrained model!\")\n",
    "\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo_agent(actor_critic=policy, ppo_epoch=16, num_mini_batch=16,\\\n",
    "                lr=0.001, eps=1e-5, max_grad_norm=0.5)\n",
    "\n",
    "rollouts = RolloutStorage(num_steps=max_steps, num_processes=num_processes, \\\n",
    "                        obs_shape=envs.observation_space.shape, action_space=envs.action_space, \\\n",
    "                        recurrent_hidden_state_size=policy.recurrent_hidden_state_size)\n",
    "\n",
    "obs = envs.reset()\n",
    "print('type obs: ', type(obs), ', shape obs: ', obs.shape)\n",
    "obs_t = torch.tensor(obs)\n",
    "print('type obs_t: ', type(obs_t), ', shape obs_t: ', obs_t.shape)\n",
    "\n",
    "rollouts.obs[0].copy_(obs_t)\n",
    "rollouts.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, directory, filename, suffix):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    torch.save(model.base.actor.state_dict(), '%s/%s_actor_%s.pth' % (directory, filename, suffix))\n",
    "    torch.save(model.base.critic.state_dict(), '%s/%s_critic_%s.pth' % (directory, filename, suffix))\n",
    "    torch.save(model.base.critic_linear.state_dict(), '%s/%s_critic_linear_%s.pth' % (directory, filename, suffix))\n",
    "\n",
    "# limits = [-300, -160, -100, -70, -50, 0, 20, 30, 40, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]\n",
    "limits = [-900, -600, -500, -400, -300, -160, -100, -70, -50, 0, 20, 30, 40, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]\n",
    "\n",
    "def return_suffix(j):\n",
    "    suf = '0'\n",
    "    for i in range(len(limits)-1):\n",
    "        if j > limits[i] and j < limits[i+1]:\n",
    "            suf = str(limits[i+1])\n",
    "            break\n",
    "        \n",
    "        i_last = len(limits)-1    \n",
    "        if  j > limits[i_last]:\n",
    "            suf = str(limits[i_last])\n",
    "            break\n",
    "    return suf      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_updates=1000000\n",
    "# num_updates=2000\n",
    "num_updates = 3\n",
    "gamma = 0.99\n",
    "tau=0.95\n",
    "save_interval=30\n",
    "log_interval= 1\n",
    "\n",
    "# Define the log directory\n",
    "log_dir = 'runs/ppo_experiment'\n",
    "\n",
    "# Get the current time as a string for tagging\n",
    "current_time_tag = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def ppo_vec_env_train(envs, agent, policy, num_processes, num_steps, rollouts):\n",
    "\n",
    "    # Create a new SummaryWriter\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    time_start = time.time()\n",
    "    \n",
    "    n=len(envs.ps)\n",
    "    envs.reset()\n",
    "    \n",
    "    # start all parallel agents\n",
    "    print('Number of agents: ', n)\n",
    "    envs.step([[1]*4]*n)\n",
    "    \n",
    "    indices = []\n",
    "    for i  in range(n):\n",
    "        indices.append(i)\n",
    "     \n",
    "    s = 0\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_array = []\n",
    "    avg_scores_array = []\n",
    "\n",
    "    for i_episode in range(num_updates):\n",
    "        \n",
    "        total_reward = np.zeros(n)\n",
    "        timestep = 0\n",
    "        \n",
    "        for timestep in range(num_steps):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                value, actions, action_log_prob, recurrent_hidden_states = \\\n",
    "                   policy.act(\n",
    "                        rollouts.obs[timestep],\n",
    "                        rollouts.recurrent_hidden_states[timestep],\n",
    "                        rollouts.masks[timestep])\n",
    "            \n",
    "            debug = True\n",
    "\n",
    "            # if debug:\n",
    "            #     print(f'{timestep} action is {actions}')\n",
    "            #     print(f'{timestep} the size of action_log_prob is {action_log_prob.size()}')    \n",
    "                \n",
    "            obs, rewards, done, infos = envs.step(actions.cpu().detach().numpy())\n",
    "            # obs, rewards, done, truncs, infos = envs.step(actions.cpu().detach().numpy())\n",
    "            \n",
    "            total_reward += rewards  ## this is the list by agents\n",
    "                        \n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            obs_t = torch.tensor(obs)\n",
    "            ## Add one dimnesion to tensor, otherwise does not work\n",
    "            ## This is (unsqueeze(1)) solution for:\n",
    "            ## RuntimeError: The expanded size of the tensor (1) must match the existing size...\n",
    "            rewards_t = torch.tensor(rewards).unsqueeze(1)\n",
    "            rollouts.insert(obs_t, recurrent_hidden_states, actions, action_log_prob, \\\n",
    "                value, rewards_t, masks)\n",
    "                    \n",
    "        avg_total_reward = np.mean(total_reward)\n",
    "        scores_deque.append(avg_total_reward)\n",
    "        scores_array.append(avg_total_reward)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            next_value = policy.get_value(rollouts.obs[-1],\n",
    "                            rollouts.recurrent_hidden_states[-1],\n",
    "                            rollouts.masks[-1]).detach()\n",
    "\n",
    "        rollouts.compute_returns(next_value, gamma, tau)\n",
    "\n",
    "        agent.update(rollouts)\n",
    "\n",
    "        rollouts.after_update()\n",
    "        \n",
    "        avg_score = np.mean(scores_deque)\n",
    "        avg_scores_array.append(avg_score)\n",
    "\n",
    "        # TensorBoard Logging\n",
    "        writer.add_scalar(f'Average Total Reward/{current_time_tag}', avg_total_reward, i_episode)\n",
    "        writer.add_scalar(f'Average Score/{current_time_tag}', avg_score, i_episode)\n",
    "        \n",
    "        if i_episode > 0 and i_episode % save_interval == 0:\n",
    "            print('Saving model, i_episode: ', i_episode, '\\n')\n",
    "            suf = return_suffix(avg_score)\n",
    "            save(policy, 'dir_save', 'we0', suf)\n",
    "\n",
    "        \n",
    "        if i_episode % log_interval == 0 and len(scores_deque) > 1:            \n",
    "            prev_s = s\n",
    "            s = (int)(time.time() - time_start)\n",
    "            t_del = s - prev_s\n",
    "            print('Ep. {}, Timesteps {}, Score.Agents: {:.2f}, Avg.Score: {:.2f}, Time: {:02}:{:02}:{:02}, \\\n",
    "Interval: {:02}:{:02}'\\\n",
    "                   .format(i_episode, timestep+1, \\\n",
    "                        avg_total_reward, avg_score, s//3600, s%3600//60, s%60, t_del%3600//60, t_del%60)) \n",
    "\n",
    "    \n",
    "        # if len(scores_deque) == 100 and np.mean(scores_deque) > 300.5:   \n",
    "        # if len(scores_deque) == 100 and np.mean(scores_deque) > 300.0:   \n",
    "        if len(scores_deque) == 100 and np.mean(scores_deque) > 100:\n",
    "        # if np.mean(scores_deque) > 20:   \n",
    "            print('Environment solved with Average Score: ',  np.mean(scores_deque) )\n",
    "            break\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "    return scores_array, avg_scores_array\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores, avg_scores = ppo_vec_env_train(envs, agent, policy, num_processes, max_steps, rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('length of scores: ', len(scores), ', len of avg_scores: ', len(avg_scores))\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# plt.plot(np.arange(1, len(scores)+1), scores, label=\"Score\")\n",
    "# plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1)) \n",
    "# plt.ylabel('Score')\n",
    "# plt.xlabel('Episodes #')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- make_vec_envs ----------------\n",
    "## we continue with the same model, model Policy uses MLPBase, but with new environment env_venv\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "seed = 0 \n",
    "num_processes=1\n",
    "\n",
    "env_venv = make_vec_envs('Group24M4-v0', \\\n",
    "                    seed + 1000, num_processes,\n",
    "                    None, None, False, device='cpu', allow_early_resets=False)\n",
    "\n",
    "policy = policy.to(device)\n",
    "\n",
    "print('env_venv.observation_space.shape: ', env_venv.observation_space.shape, \\\n",
    "      ', len(obs_shape): ', len(env_venv.observation_space.shape))\n",
    "print('env_venv.action_space: ',  env_venv.action_space, \\\n",
    "      ', action_space.shape[0]: ', env_venv.action_space.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No CUDA, only CPU\n",
    "\n",
    "def play_VecEnv(env, model, num_episodes):\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param)\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = torch.Tensor(obs)\n",
    "    obs = obs.float()\n",
    "        \n",
    "    recurrent_hidden_states = torch.zeros(1, model.recurrent_hidden_state_size)\n",
    "    \n",
    "    masks = torch.zeros(1, 1)\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "\n",
    "    render_func = get_render_func(env)\n",
    "        \n",
    "    for i_episode in range(1, num_episodes+1):     \n",
    "\n",
    "        time_start = time.time()\n",
    "        total_reward = np.zeros(num_processes)\n",
    "        timestep = 0\n",
    "\n",
    "        while True:\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                value, action, _, recurrent_hidden_states = \\\n",
    "                    model.act(obs, recurrent_hidden_states, masks, \\\n",
    "                    deterministic=False) # obs = state\n",
    "\n",
    "            render_func()\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            obs = torch.Tensor(obs)\n",
    "            obs = obs.float()\n",
    "\n",
    "            reward = reward.detach().numpy()\n",
    "            \n",
    "            masks.fill_(0.0 if done else 1.0)\n",
    "            \n",
    "            total_reward += reward[0]\n",
    "        \n",
    "            #if timestep < 800:\n",
    "            #    print('timestep: ', timestep, 'reward: ', reward[0])\n",
    "            \n",
    "            timestep += 1\n",
    "            \n",
    "            if timestep + 1 == 1600: ##   envs.max_steps:\n",
    "                break\n",
    "\n",
    "        s = (int)(time.time() - time_start)\n",
    "        \n",
    "        scores_deque.append(total_reward[0])\n",
    "        \n",
    "        avg_score = np.mean(scores_deque)\n",
    "                    \n",
    "        print('Episode {} \\tScore: {:.2f}, Avg.Score: {:.2f}, \\tTime: {:02}:{:02}:{:02}'\\\n",
    "                  .format(i_episode, total_reward[0], avg_score,  s//3600, s%3600//60, s%60))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_VecEnv(env=env_venv, model=policy, num_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_venv.close()\n",
    "obs = env_venv.reset()\n",
    "env_venv.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n5418",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
