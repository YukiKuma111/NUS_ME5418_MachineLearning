{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Mobility Morphobot (M4) with PPO, Vectorized Environment\n",
    "In this notebook, you will implement a PPO agent with Group24M4-v0 environment.\n",
    "\n",
    "### 1. Create Vectorized Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:27:41.220495Z",
     "start_time": "2024-11-03T15:27:40.911502Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from parallelEnv import parallelEnv\n",
    "from model import PolicyNetwork as actor\n",
    "from model import ValueNetwork as critic\n",
    "from ppo import ppo_agent\n",
    "from storage import RolloutStorage\n",
    "from gym.vector import SyncVectorEnv\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from envs import make_vec_envs\n",
    "from utils import get_render_func\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "debug = False\n",
    "\n",
    "print('gym version: ', gym.__version__)\n",
    "print('torch version: ', torch.__version__)\n",
    "print('cuda available: ', torch.cuda.is_available())\n",
    "\n",
    "seed = 0 \n",
    "gamma=0.99\n",
    "num_processes=16 \n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "print('device: ', device)\n",
    "\n",
    "envs = parallelEnv('Group24M4-v0', n=num_processes, seed=seed)\n",
    "\n",
    "max_steps = envs.max_steps\n",
    "print('max_steps: ', max_steps)\n",
    "\n",
    "if debug:\n",
    "    action = envs.action_space.sample()\n",
    "    observation = envs.observation_space.sample()\n",
    "    ac_size = envs.action_space.shape[0]\n",
    "    ob_size = envs.observation_space.shape[0]\n",
    "\n",
    "    print(\"action\", action)\n",
    "    print(\"observation\", observation)\n",
    "    print(\"ac_size\", ac_size)\n",
    "    print(\"ob_size\", ob_size)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate Model, Agent and Storage\n",
    "\n",
    "Initialize the Policy (model PolicyNetwork), PPO Agent and Rollout Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:27:44.430108Z",
     "start_time": "2024-11-03T15:27:44.325132Z"
    }
   },
   "outputs": [],
   "source": [
    "# init the nn\n",
    "hidden_size_gru = 35\n",
    "hidden_size_MLP = 64\n",
    "\n",
    "if debug:\n",
    "    print(\"envs.observation_space.shape: \", envs.observation_space.shape[0])\n",
    "    print(\"envs.action_space: \", envs.action_space.shape[0])\n",
    "    \n",
    "# actor\n",
    "actor_nn = actor(envs.observation_space.shape[0],hidden_size_gru,hidden_size_MLP, envs.action_space.shape[0])\n",
    "# critic\n",
    "critic_nn = critic(envs.observation_space.shape[0], hidden_size_gru,hidden_size_MLP)\n",
    "    \n",
    "actor_nn.to(device)\n",
    "critic_nn.to(device)\n",
    "\n",
    "agent = ppo_agent(actor=actor_nn,critic=critic_nn, ppo_epoch=16, num_mini_batch=16,lr=0.001, eps=1e-5, max_grad_norm=0.5)\n",
    "\n",
    "# init the storage used to store the params of nn, state, action, rewards\n",
    "rollouts = RolloutStorage(num_steps=max_steps, num_processes=num_processes, obs_shape=envs.observation_space.shape, action_space=envs.action_space, recurrent_hidden_state_size=hidden_size_gru)\n",
    "print('storage situation', rollouts)\n",
    "\n",
    "obs = envs.reset()\n",
    "print('type obs: ', type(obs), ', shape obs: ', obs.shape)\n",
    "obs_t = torch.tensor(obs)\n",
    "print('type obs_t: ', type(obs_t), ', shape obs_t: ', obs_t.shape)\n",
    "\n",
    "rollouts.obs[0].copy_(obs_t)\n",
    "rollouts.to(device)\n",
    "\n",
    "if debug:\n",
    "    print('action_log_prob', rollouts.action_log_probs[0])\n",
    "    print('action',rollouts.actions[0])\n",
    "    print('obs space', rollouts.obs[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Save model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:27:47.844946Z",
     "start_time": "2024-11-03T15:27:47.813398Z"
    }
   },
   "outputs": [],
   "source": [
    "def save(model_1,model_2, directory, filename, suffix):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    torch.save(model_1.state_dict(), '%s/%s_actor_%s.pth' % (directory, filename, suffix))\n",
    "    torch.save(model_2.state_dict(), '%s/%s_critic_%s.pth' % (directory, filename, suffix))\n",
    "    \n",
    "limits = [-300, -160, -100, -70, -50, 0, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360, 390, 420]\n",
    "\n",
    "def return_suffix(j):\n",
    "    suf = '0'\n",
    "    for i in range(len(limits)-1):\n",
    "        if j > limits[i] and j < limits[i+1]:\n",
    "            suf = str(limits[i+1])\n",
    "            break\n",
    "        \n",
    "        i_last = len(limits)-1    \n",
    "        if  j > limits[i_last]:\n",
    "            suf = str(limits[i_last])\n",
    "            break\n",
    "    return suf      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Agent  with Vectorized Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:27:50.860447Z",
     "start_time": "2024-11-03T15:27:50.820780Z"
    }
   },
   "outputs": [],
   "source": [
    "num_updates=1000\n",
    "gamma = 0.99\n",
    "tau=0.95\n",
    "save_interval=30\n",
    "log_interval= 1\n",
    "\n",
    "# Define the log directory\n",
    "log_dir = 'runs/ppo_experiment'\n",
    "\n",
    "# Get the current time as a string for tagging\n",
    "current_time_tag = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def ppo_vec_env_train(envs, agent, actor_nn,critic_nn, num_processes, num_steps, rollouts):\n",
    "\n",
    "    # Create a new SummaryWriter\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    time_start = time.time()\n",
    "    \n",
    "    n=len(envs.ps)\n",
    "    envs.reset()\n",
    "    \n",
    "    # start all parallel agents\n",
    "    print('Number of agents: ', n)\n",
    "    envs.step([[1]*4]*n)\n",
    "    \n",
    "    indices = []\n",
    "    for i  in range(n):\n",
    "        indices.append(i)\n",
    "     \n",
    "    s = 0\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_array = []\n",
    "    avg_scores_array = []\n",
    "\n",
    "    for i_episode in range(num_updates):\n",
    "        \n",
    "        total_reward = np.zeros(n)\n",
    "        timestep = 0\n",
    "        \n",
    "        for timestep in range(num_steps):\n",
    "            \n",
    "            if debug:\n",
    "                print(f'{timestep} obs {rollouts.obs[timestep]}')\n",
    "                print(f'{timestep} recurrent hidden states {rollouts.recurrent_hidden_states[timestep]}')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                value, actions, action_log_prob, recurrent_hidden_states = \\\n",
    "                   actor_nn.act(\n",
    "                        rollouts.obs[timestep],\n",
    "                        rollouts.recurrent_hidden_states[timestep],\n",
    "                        rollouts.masks[timestep])\n",
    "            \n",
    "            if debug:    \n",
    "                print('action',actions)\n",
    "                print('the size of action log probs',action_log_prob.size())\n",
    "                print('action log prob',action_log_prob)\n",
    "                \n",
    "            obs, rewards, done, infos = envs.step(actions.cpu().detach().numpy())\n",
    "            \n",
    "            total_reward += rewards  ## this is the list by agents\n",
    "                        \n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            obs_t = torch.tensor(obs)\n",
    "            ## Add one dimnesion to tensor, otherwise does not work\n",
    "            ## This is (unsqueeze(1)) solution for:\n",
    "            ## RuntimeError: The expanded size of the tensor (1) must match the existing size...\n",
    "            rewards_t = torch.tensor(rewards).unsqueeze(1)\n",
    "            rollouts.insert(obs_t, recurrent_hidden_states, actions, action_log_prob, \\\n",
    "                value, rewards_t, masks)\n",
    "                    \n",
    "        avg_total_reward = np.mean(total_reward)\n",
    "        scores_deque.append(avg_total_reward)\n",
    "        scores_array.append(avg_total_reward)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            next_value = critic_nn.get_value(rollouts.obs[-1],\n",
    "                            rollouts.recurrent_hidden_states[-1],\n",
    "                            rollouts.masks[-1]).detach()\n",
    "\n",
    "        rollouts.compute_returns(next_value, gamma, tau)\n",
    "\n",
    "        agent.update(rollouts)\n",
    "\n",
    "        rollouts.after_update()\n",
    "        \n",
    "        avg_score = np.mean(scores_deque)\n",
    "        avg_scores_array.append(avg_score)\n",
    "\n",
    "        # TensorBoard Logging\n",
    "        writer.add_scalar(f'Average Total Reward/{current_time_tag}', avg_total_reward, i_episode)\n",
    "        writer.add_scalar(f'Average Score/{current_time_tag}', avg_score, i_episode)\n",
    "        \n",
    "        if i_episode > 0 and i_episode % save_interval == 0:\n",
    "            print('Saving model, i_episode: ', i_episode, '\\n')\n",
    "            suf = return_suffix(avg_score)\n",
    "            save(actor_nn,critic_nn, 'dir_save', 'we0', suf)\n",
    "\n",
    "        \n",
    "        if i_episode % log_interval == 0 and len(scores_deque) > 1:            \n",
    "            prev_s = s\n",
    "            s = (int)(time.time() - time_start)\n",
    "            t_del = s - prev_s\n",
    "            print('Ep. {}, Timesteps {}, Score.Agents: {:.2f}, Avg.Score: {:.2f}, Time: {:02}:{:02}:{:02}, \\\n",
    "Interval: {:02}:{:02}'\\\n",
    "                   .format(i_episode, timestep+1, \\\n",
    "                        avg_total_reward, avg_score, s//3600, s%3600//60, s%60, t_del%3600//60, t_del%60)) \n",
    "\n",
    "    \n",
    "        if len(scores_deque) == 100 and np.mean(scores_deque) > 1100:\n",
    "            print('Environment solved with Average Score: ',  np.mean(scores_deque) )\n",
    "            break\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "    return scores_array, avg_scores_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:28:51.539503Z",
     "start_time": "2024-11-03T15:27:57.914526Z"
    }
   },
   "outputs": [],
   "source": [
    "scores, avg_scores = ppo_vec_env_train(envs, agent, actor_nn, critic_nn, num_processes, max_steps, rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:29:02.389448Z",
     "start_time": "2024-11-03T15:29:02.360818Z"
    }
   },
   "outputs": [],
   "source": [
    "save(model_1= actor_nn, model_2= critic_nn, directory='dir_save', filename='we0', suffix='final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:29:07.048725Z",
     "start_time": "2024-11-03T15:29:06.298974Z"
    }
   },
   "outputs": [],
   "source": [
    "print('length of scores: ', len(scores), ', len of avg_scores: ', len(avg_scores))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores, label=\"Score\")\n",
    "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1)) \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:29:15.204772Z",
     "start_time": "2024-11-03T15:29:12.992042Z"
    }
   },
   "outputs": [],
   "source": [
    "#--------------- make_vec_envs ----------------\n",
    "## we continue with the same model, model Policy uses MLPBase, but with new environment env_venv\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "seed = 0 \n",
    "num_processes=1\n",
    "\n",
    "env_venv = make_vec_envs('Group24M4-v0', \\\n",
    "                    seed + 1000, num_processes,\n",
    "                    None, None, False, device='cpu', allow_early_resets=False)\n",
    "\n",
    "actor_nn = actor_nn.to(device)\n",
    "critic_nn = critic_nn.to(device)\n",
    "\n",
    "print('env_venv.observation_space.shape: ', env_venv.observation_space.shape, \\\n",
    "      ', len(obs_shape): ', len(env_venv.observation_space.shape))\n",
    "print('env_venv.action_space: ',  env_venv.action_space, \\\n",
    "      ', action_space.shape[0]: ', env_venv.action_space.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test and display training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:32:05.019842Z",
     "start_time": "2024-11-03T15:32:04.993398Z"
    }
   },
   "outputs": [],
   "source": [
    "## No CUDA, only CPU\n",
    "def play_VecEnv(env, actor_nn, critic_nn, num_episodes, recurrent_hidden_state_size):\n",
    "\n",
    "    # for name, param in actor_nn.named_parameters():\n",
    "    #     print(name, param)\n",
    "        \n",
    "    # for name, param in critic_nn.named_parameters():\n",
    "    #     print(name, param)\n",
    "        \n",
    "    obs = env.reset()\n",
    "    obs = torch.Tensor(obs)\n",
    "    obs = obs.float()\n",
    "    \n",
    "    recurrent_hidden_states = torch.zeros(1, recurrent_hidden_state_size)\n",
    "    \n",
    "    masks = torch.ones(1, recurrent_hidden_state_size,dtype=torch.float32)\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "\n",
    "    render_func = get_render_func(env)\n",
    "        \n",
    "    for i_episode in range(1, num_episodes+1):     \n",
    "\n",
    "        time_start = time.time()\n",
    "        total_reward = np.zeros(num_processes)\n",
    "        timestep = 0\n",
    "\n",
    "        while True:\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                value, action, _, recurrent_hidden_states = \\\n",
    "                    actor_nn.act(obs, recurrent_hidden_states, masks, \\\n",
    "                    deterministic=False) # obs = state\n",
    "\n",
    "            render_func()\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            obs = torch.Tensor(obs)\n",
    "            obs = obs.float()\n",
    "\n",
    "            reward = reward.detach().numpy()\n",
    "            \n",
    "            masks.fill_(0.0 if done else 1.0)\n",
    "            \n",
    "            total_reward += reward[0]\n",
    "\n",
    "            timestep += 1\n",
    "            \n",
    "            if timestep + 1 == 1600: ##   envs.max_steps:\n",
    "                break\n",
    "\n",
    "        s = (int)(time.time() - time_start)\n",
    "        \n",
    "        scores_deque.append(total_reward[0])\n",
    "        \n",
    "        avg_score = np.mean(scores_deque)\n",
    "                    \n",
    "        print('Episode {} \\tScore: {:.2f}, Avg.Score: {:.2f}, \\tTime: {:02}:{:02}:{:02}'\\\n",
    "                  .format(i_episode, total_reward[0], avg_score,  s//3600, s%3600//60, s%60))  \n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:38:09.499997Z",
     "start_time": "2024-11-03T15:33:09.188321Z"
    }
   },
   "outputs": [],
   "source": [
    "play_VecEnv(env=env_venv, actor_nn=actor_nn, critic_nn=critic_nn, num_episodes=5,recurrent_hidden_state_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T15:38:12.323243Z",
     "start_time": "2024-11-03T15:38:12.253052Z"
    }
   },
   "outputs": [],
   "source": [
    "env_venv.close()\n",
    "obs = env_venv.reset()\n",
    "env_venv.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n5418",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
